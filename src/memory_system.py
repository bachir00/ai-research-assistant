"""
SystÃ¨me de MÃ©moire et Stockage Vectoriel pour l'Assistant de Recherche
GÃ¨re : embeddings, recherche sÃ©mantique, historique et dÃ©duplication
"""

import chromadb
from chromadb.config import Settings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import hashlib
import json
import pickle
from pathlib import Path
from collections import deque

# ============================================================================
# GESTIONNAIRE DE MÃ‰MOIRE VECTORIELLE
# ============================================================================

class VectorMemoryManager:
    """GÃ¨re le stockage vectoriel des documents et rÃ©sumÃ©s"""
    
    def __init__(self, 
                 persist_directory: str = "./chroma_db",
                 collection_name: str = "research_documents",
                 embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        Initialise le gestionnaire de mÃ©moire vectorielle
        
        Args:
            persist_directory: Dossier de persistance de ChromaDB
            collection_name: Nom de la collection ChromaDB
            embedding_model: ModÃ¨le d'embeddings HuggingFace
        """
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(parents=True, exist_ok=True)
        
        print(f"ðŸ”§ Initialisation du systÃ¨me de mÃ©moire vectorielle...")
        
        # Configuration des embeddings
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # Configuration ChromaDB
        self.client = chromadb.PersistentClient(
            path=str(self.persist_directory),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # CrÃ©er ou rÃ©cupÃ©rer la collection
        try:
            self.collection = self.client.get_collection(collection_name)
            print(f"âœ… Collection '{collection_name}' rÃ©cupÃ©rÃ©e ({self.collection.count()} documents)")
        except:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            print(f"âœ… Nouvelle collection '{collection_name}' crÃ©Ã©e")
        
        # Initialiser le vectorstore LangChain
        self.vectorstore = Chroma(
            client=self.client,
            collection_name=collection_name,
            embedding_function=self.embeddings
        )
        
        # Cache pour dÃ©duplication rapide
        self.content_hashes = set()
        self._load_existing_hashes()
    
    def _load_existing_hashes(self):
        """Charge les hashes des documents existants pour dÃ©duplication"""
        try:
            results = self.collection.get(include=['metadatas'])
            for metadata in results['metadatas']:
                if 'content_hash' in metadata:
                    self.content_hashes.add(metadata['content_hash'])
            print(f"ðŸ“‹ {len(self.content_hashes)} hashes chargÃ©s pour dÃ©duplication")
        except Exception as e:
            print(f"âš ï¸ Erreur lors du chargement des hashes: {e}")
    
    def _compute_hash(self, content: str) -> str:
        """Calcule le hash MD5 d'un contenu"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def is_duplicate(self, content: str) -> bool:
        """VÃ©rifie si un document est un doublon"""
        content_hash = self._compute_hash(content)
        return content_hash in self.content_hashes
    
    def add_documents(self, 
                     documents: List[Dict[str, any]], 
                     source: str = "research",
                     check_duplicates: bool = True) -> Dict[str, int]:
        """
        Ajoute des documents au vectorstore
        
        Args:
            documents: Liste de dicts avec 'content', 'title', 'url', etc.
            source: Source des documents (research, summary, synthesis)
            check_duplicates: VÃ©rifier les doublons avant ajout
        
        Returns:
            Dict avec statistiques d'ajout
        """
        print(f"\nðŸ“¥ Ajout de {len(documents)} documents (source: {source})...")
        
        added = 0
        skipped = 0
        
        docs_to_add = []
        metadatas_to_add = []
        ids_to_add = []
        
        for doc in documents:
            content = doc.get('content', '')
            
            # VÃ©rification des doublons
            if check_duplicates and self.is_duplicate(content):
                skipped += 1
                continue
            
            # CrÃ©ation du document LangChain
            content_hash = self._compute_hash(content)
            doc_id = f"{source}_{content_hash[:8]}_{datetime.now().timestamp()}"
            
            metadata = {
                'title': doc.get('title', 'Sans titre'),
                'url': doc.get('url', ''),
                'source': source,
                'timestamp': datetime.now().isoformat(),
                'content_hash': content_hash,
                'word_count': len(content.split())
            }
            
            docs_to_add.append(content)
            metadatas_to_add.append(metadata)
            ids_to_add.append(doc_id)
            self.content_hashes.add(content_hash)
            added += 1
        
        # Ajout batch Ã  ChromaDB
        if docs_to_add:
            self.collection.add(
                documents=docs_to_add,
                metadatas=metadatas_to_add,
                ids=ids_to_add
            )
        
        stats = {
            'added': added,
            'skipped': skipped,
            'total_in_db': self.collection.count()
        }
        
        print(f"âœ… AjoutÃ©s: {added} | Doublons ignorÃ©s: {skipped} | Total DB: {stats['total_in_db']}")
        return stats
    
    def semantic_search(self, 
                       query: str, 
                       k: int = 5,
                       filter_dict: Optional[Dict] = None) -> List[Tuple[Document, float]]:
        """
        Recherche sÃ©mantique dans le vectorstore
        
        Args:
            query: RequÃªte de recherche
            k: Nombre de rÃ©sultats Ã  retourner
            filter_dict: Filtres sur les mÃ©tadonnÃ©es (ex: {'source': 'research'})
        
        Returns:
            Liste de tuples (Document, score)
        """
        print(f"\nðŸ” Recherche sÃ©mantique: '{query}' (top-{k})")
        
        results = self.vectorstore.similarity_search_with_score(
            query=query,
            k=k,
            filter=filter_dict
        )
        
        print(f"âœ… {len(results)} rÃ©sultats trouvÃ©s")
        return results
    
    def get_relevant_context(self, 
                            query: str, 
                            k: int = 3,
                            source_filter: Optional[str] = None) -> str:
        """
        RÃ©cupÃ¨re le contexte pertinent pour une requÃªte
        
        Args:
            query: RequÃªte
            k: Nombre de documents Ã  rÃ©cupÃ©rer
            source_filter: Filtrer par source (research, summary, etc.)
        
        Returns:
            Contexte formatÃ© en string
        """
        filter_dict = {"source": source_filter} if source_filter else None
        results = self.semantic_search(query, k=k, filter_dict=filter_dict)
        
        if not results:
            return ""
        
        context_parts = []
        for i, (doc, score) in enumerate(results, 1):
            context_parts.append(
                f"[Source {i} - Pertinence: {score:.2f}]\n"
                f"Titre: {doc.metadata.get('title', 'N/A')}\n"
                f"{doc.page_content[:500]}...\n"
            )
        
        return "\n---\n".join(context_parts)
    
    def clear_old_documents(self, days: int = 30) -> int:
        """
        Supprime les documents plus anciens que X jours
        
        Args:
            days: Nombre de jours de rÃ©tention
        
        Returns:
            Nombre de documents supprimÃ©s
        """
        print(f"\nðŸ§¹ Nettoyage des documents > {days} jours...")
        
        from datetime import timedelta
        cutoff_date = datetime.now() - timedelta(days=days)
        
        results = self.collection.get(include=['metadatas'])
        ids_to_delete = []
        
        for doc_id, metadata in zip(results['ids'], results['metadatas']):
            timestamp_str = metadata.get('timestamp', '')
            try:
                doc_date = datetime.fromisoformat(timestamp_str)
                if doc_date < cutoff_date:
                    ids_to_delete.append(doc_id)
                    hash_to_remove = metadata.get('content_hash')
                    if hash_to_remove:
                        self.content_hashes.discard(hash_to_remove)
            except:
                continue
        
        if ids_to_delete:
            self.collection.delete(ids=ids_to_delete)
        
        print(f"âœ… {len(ids_to_delete)} documents supprimÃ©s")
        return len(ids_to_delete)


# ============================================================================
# GESTIONNAIRE DE MÃ‰MOIRE D'AGENT
# ============================================================================

class AgentMemoryManager:
    """GÃ¨re l'historique des conversations et rÃ©sumÃ©s"""
    
    def __init__(self, 
                 memory_file: str = "./agent_memory.pkl",
                 max_history: int = 100,
                 compression_threshold: int = 50):
        """
        Initialise le gestionnaire de mÃ©moire d'agent
        
        Args:
            memory_file: Fichier de sauvegarde de la mÃ©moire
            max_history: Nombre maximum d'entrÃ©es dans l'historique
            compression_threshold: Seuil pour compression de mÃ©moire
        """
        self.memory_file = Path(memory_file)
        self.max_history = max_history
        self.compression_threshold = compression_threshold
        
        # Structures de donnÃ©es
        self.conversation_history = deque(maxlen=max_history)
        self.research_cache = {}  # topic -> result
        self.summary_cache = {}    # topic -> summary
        self.topic_keywords = {}   # topic -> keywords
        
        print(f"ðŸ§  Initialisation du gestionnaire de mÃ©moire d'agent...")
        self._load_memory()
    
    def _load_memory(self):
        """Charge la mÃ©moire depuis le fichier"""
        if self.memory_file.exists():
            try:
                with open(self.memory_file, 'rb') as f:
                    data = pickle.load(f)
                    self.conversation_history = data.get('conversation_history', deque(maxlen=self.max_history))
                    self.research_cache = data.get('research_cache', {})
                    self.summary_cache = data.get('summary_cache', {})
                    self.topic_keywords = data.get('topic_keywords', {})
                print(f"âœ… MÃ©moire chargÃ©e: {len(self.conversation_history)} conversations, "
                      f"{len(self.research_cache)} recherches en cache")
            except Exception as e:
                print(f"âš ï¸ Erreur lors du chargement de la mÃ©moire: {e}")
        else:
            print("â„¹ï¸ Nouvelle mÃ©moire initialisÃ©e")
    
    def _save_memory(self):
        """Sauvegarde la mÃ©moire dans le fichier"""
        try:
            data = {
                'conversation_history': self.conversation_history,
                'research_cache': self.research_cache,
                'summary_cache': self.summary_cache,
                'topic_keywords': self.topic_keywords
            }
            with open(self.memory_file, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            print(f"âš ï¸ Erreur lors de la sauvegarde de la mÃ©moire: {e}")
    
    def add_conversation(self, user_message: str, assistant_response: str, metadata: Optional[Dict] = None):
        """Ajoute une conversation Ã  l'historique"""
        entry = {
            'timestamp': datetime.now().isoformat(),
            'user': user_message,
            'assistant': assistant_response,
            'metadata': metadata or {}
        }
        self.conversation_history.append(entry)
        
        # Compression si nÃ©cessaire
        if len(self.conversation_history) >= self.compression_threshold:
            self._compress_memory()
        
        self._save_memory()
    
    def add_research_result(self, topic: str, result: any, keywords: List[str]):
        """Cache un rÃ©sultat de recherche"""
        self.research_cache[topic] = {
            'result': result,
            'timestamp': datetime.now().isoformat()
        }
        self.topic_keywords[topic] = keywords
        self._save_memory()
    
    def get_research_result(self, topic: str, max_age_hours: int = 24) -> Optional[any]:
        """RÃ©cupÃ¨re un rÃ©sultat de recherche en cache"""
        if topic not in self.research_cache:
            return None
        
        cached = self.research_cache[topic]
        cached_time = datetime.fromisoformat(cached['timestamp'])
        
        from datetime import timedelta
        if datetime.now() - cached_time > timedelta(hours=max_age_hours):
            print(f"â„¹ï¸ Cache expirÃ© pour '{topic}'")
            return None
        
        print(f"âœ… RÃ©sultat rÃ©cupÃ©rÃ© du cache pour '{topic}'")
        return cached['result']
    
    def add_summary(self, topic: str, summary: str):
        """Ajoute un rÃ©sumÃ© au cache"""
        self.summary_cache[topic] = {
            'summary': summary,
            'timestamp': datetime.now().isoformat()
        }
        self._save_memory()
    
    def get_conversation_context(self, n_last: int = 5) -> str:
        """RÃ©cupÃ¨re le contexte des N derniÃ¨res conversations"""
        recent = list(self.conversation_history)[-n_last:]
        
        if not recent:
            return ""
        
        context = "Contexte des conversations rÃ©centes:\n"
        for i, conv in enumerate(recent, 1):
            context += f"\n[Conversation {i}]\n"
            context += f"User: {conv['user'][:100]}...\n"
            context += f"Assistant: {conv['assistant'][:100]}...\n"
        
        return context
    
    def _compress_memory(self):
        """Compresse la mÃ©moire en gardant seulement les Ã©lÃ©ments importants"""
        print("ðŸ—œï¸ Compression de la mÃ©moire...")
        
        # Supprimer les anciennes recherches en cache (> 7 jours)
        from datetime import timedelta
        cutoff = datetime.now() - timedelta(days=7)
        
        topics_to_remove = []
        for topic, data in self.research_cache.items():
            if datetime.fromisoformat(data['timestamp']) < cutoff:
                topics_to_remove.append(topic)
        
        for topic in topics_to_remove:
            del self.research_cache[topic]
            if topic in self.topic_keywords:
                del self.topic_keywords[topic]
        
        print(f"âœ… {len(topics_to_remove)} anciennes recherches supprimÃ©es")
        self._save_memory()
    
    def get_related_topics(self, topic: str, threshold: float = 0.5) -> List[str]:
        """Trouve les topics similaires dans l'historique"""
        from difflib import SequenceMatcher
        
        related = []
        for cached_topic in self.research_cache.keys():
            similarity = SequenceMatcher(None, topic.lower(), cached_topic.lower()).ratio()
            if similarity > threshold:
                related.append((cached_topic, similarity))
        
        return [t for t, _ in sorted(related, key=lambda x: x[1], reverse=True)]
    
    def clear_all(self):
        """RÃ©initialise complÃ¨tement la mÃ©moire"""
        print("ðŸ—‘ï¸ RÃ©initialisation complÃ¨te de la mÃ©moire...")
        self.conversation_history.clear()
        self.research_cache.clear()
        self.summary_cache.clear()
        self.topic_keywords.clear()
        self._save_memory()
        print("âœ… MÃ©moire rÃ©initialisÃ©e")


# ============================================================================
# GESTIONNAIRE INTÃ‰GRÃ‰
# ============================================================================

class IntegratedMemorySystem:
    """SystÃ¨me de mÃ©moire intÃ©grÃ© combinant vectoriel et agent"""
    
    def __init__(self):
        self.vector_memory = VectorMemoryManager()
        self.agent_memory = AgentMemoryManager()
        print("âœ¨ SystÃ¨me de mÃ©moire intÃ©grÃ© initialisÃ©\n")
    
    def process_research_result(self, 
                               topic: str, 
                               extraction_result: any,
                               summarization_result: any,
                               global_synthesis: any):
        """
        Traite et stocke tous les rÃ©sultats d'une recherche
        
        Args:
            topic: Sujet de la recherche
            extraction_result: RÃ©sultat de l'extraction
            summarization_result: RÃ©sultat des rÃ©sumÃ©s
            global_synthesis: SynthÃ¨se globale
        """
        print(f"\nðŸ’¾ Stockage des rÃ©sultats pour '{topic}'...")
        
        # 1. Stocker les documents extraits dans le vectorstore
        if extraction_result and hasattr(extraction_result, 'documents'):
            docs_to_store = []
            for doc in extraction_result.documents:
                docs_to_store.append({
                    'content': doc.content,
                    'title': doc.title,
                    'url': str(doc.url)
                })
            self.vector_memory.add_documents(docs_to_store, source='research')
        
        # 2. Stocker les rÃ©sumÃ©s
        if summarization_result and hasattr(summarization_result, 'summaries'):
            summaries_to_store = []
            for summary in summarization_result.summaries:
                summaries_to_store.append({
                    'content': summary.detailed_summary,
                    'title': summary.title,
                    'url': str(summary.url)
                })
            self.vector_memory.add_documents(summaries_to_store, source='summary')
        
        # 3. Stocker la synthÃ¨se globale
        if global_synthesis and hasattr(global_synthesis, 'final_report'):
            synthesis_text = global_synthesis.final_report.formatted_outputs.get('text', '')
            self.vector_memory.add_documents([{
                'content': synthesis_text,
                'title': f"SynthÃ¨se: {topic}",
                'url': ''
            }], source='synthesis')
        
        # 4. Mettre en cache dans la mÃ©moire agent
        keywords = []
        if hasattr(extraction_result, 'documents'):
            # Extraire quelques mots-clÃ©s simples
            all_text = ' '.join([doc.content[:100] for doc in extraction_result.documents[:3]])
            keywords = list(set(all_text.split()[:10]))
        
        self.agent_memory.add_research_result(topic, global_synthesis, keywords)
        
        print("âœ… Tous les rÃ©sultats stockÃ©s avec succÃ¨s")
    
    def retrieve_context_for_query(self, query: str, use_cache: bool = True) -> Dict:
        """
        RÃ©cupÃ¨re le contexte pertinent pour une requÃªte
        
        Args:
            query: RequÃªte de l'utilisateur
            use_cache: Utiliser le cache si disponible
        
        Returns:
            Dict avec le contexte vectoriel et conversationnel
        """
        context = {
            'semantic_context': '',
            'conversation_context': '',
            'cached_result': None,
            'related_topics': []
        }
        
        # 1. VÃ©rifier le cache
        if use_cache:
            context['cached_result'] = self.agent_memory.get_research_result(query)
        
        # 2. Recherche sÃ©mantique
        context['semantic_context'] = self.vector_memory.get_relevant_context(query, k=3)
        
        # 3. Contexte conversationnel
        context['conversation_context'] = self.agent_memory.get_conversation_context(n_last=3)
        
        # 4. Topics similaires
        context['related_topics'] = self.agent_memory.get_related_topics(query)
        
        return context


# ============================================================================
# INITIALISATION GLOBALE
# ============================================================================

# Instance globale du systÃ¨me de mÃ©moire
memory_system = IntegratedMemorySystem()

print("="*60)
print("âœ… SYSTÃˆME DE MÃ‰MOIRE PRÃŠT")
print("="*60)