"""
Service LLM pour l'int√©gration avec Groq et autres fournisseurs.
G√®re les appels aux mod√®les de langage pour le r√©sum√© et l'analyse.
"""

import asyncio
import aiohttp
import json
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import time

from config.settings import api_config
from src.core.logging import setup_logger
import traceback


class LLMError(Exception):
    """Exception pour les erreurs LLM."""
    pass


class LLMRateLimitError(LLMError):
    """Exception pour les erreurs de limite de taux."""
    pass


class LLMService:
    """
    Service pour les appels aux mod√®les de langage.
    
    Fonctionnalit√©s:
    - Support de Groq API
    - Gestion des limites de taux
    - Retry automatique avec backoff
    - Streaming optionnel
    - Validation des r√©ponses
    """
    
    def __init__(self):
        self.config = api_config
        self.logger = setup_logger("llm_service")
        
        # Configuration Groq
        self.groq_api_key = self.config.GROQ_API_KEY
        self.groq_base_url = "https://api.groq.com/openai/v1"
        self.default_model = getattr(self.config, 'GROQ_MODEL', "llama-3.1-8b-instant")
        
        # Gestion des limites de taux
        self.rate_limit_requests = 30  # Requ√™tes par minute
        self.rate_limit_tokens = 6000  # Tokens par minute
        self.request_timestamps = []
        
        # Configuration par d√©faut
        self.default_params = {
            "temperature": 0.3,
            "max_tokens": 2000,
            "top_p": 0.9,
            "frequency_penalty": 0.1,
            "presence_penalty": 0.1
        }
        
        # Headers pour les requ√™tes
        self.headers = {
            "Authorization": f"Bearer {self.groq_api_key}",
            "Content-Type": "application/json"
        }
    
    async def generate_completion(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        G√©n√®re une compl√©tion de texte.
        
        Args:
            prompt: Prompt utilisateur
            system_prompt: Prompt syst√®me optionnel
            model: Mod√®le √† utiliser (d√©faut: config)
            **kwargs: Param√®tres suppl√©mentaires pour l'API
            
        Returns:
            R√©ponse g√©n√©r√©e par le mod√®le
            
        Raises:
            LLMError: En cas d'erreur API
            LLMRateLimitError: En cas de d√©passement de limite
        """
        # Pr√©parer les messages
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # Param√®tres de la requ√™te
        params = {**self.default_params, **kwargs}
        payload = {
            "model": model or self.default_model,
            "messages": messages,
            **params
        }
        
        # Gestion des limites de taux
        await self._check_rate_limits()
        
        # Appel API avec retry
        return await self._make_api_call(payload)
    
    async def generate_batch_completions(
        self,
        prompts: List[str],
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        max_concurrent: int = 3,
        **kwargs
    ) -> List[str]:
        """
        G√©n√®re plusieurs compl√©tions en parall√®le.
        
        Args:
            prompts: Liste des prompts
            system_prompt: Prompt syst√®me optionnel
            model: Mod√®le √† utiliser
            max_concurrent: Nombre maximum de requ√™tes simultan√©es
            **kwargs: Param√®tres suppl√©mentaires
            
        Returns:
            Liste des r√©ponses dans le m√™me ordre que les prompts
        """
        self.logger.info(f"G√©n√©ration batch de {len(prompts)} compl√©tions")
        
        # Cr√©er un semaphore pour limiter la concurrence
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def generate_single(prompt: str, index: int) -> tuple:
            async with semaphore:
                try:
                    # D√©lai pour √©viter le rate limiting
                    await asyncio.sleep(index * 0.5)
                    
                    result = await self.generate_completion(
                        prompt, system_prompt, model, **kwargs
                    )
                    return index, result
                except Exception as e:
                    self.logger.error(f"Erreur completion {index}: {e}")
                    return index, f"ERREUR: {str(e)}"
        
        # Lancer toutes les t√¢ches
        tasks = [generate_single(prompt, i) for i, prompt in enumerate(prompts)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # R√©organiser les r√©sultats dans l'ordre
        ordered_results = [""] * len(prompts)
        for result in results:
            if isinstance(result, tuple):
                index, content = result
                ordered_results[index] = content
            else:
                # Exception - la placer √† la fin
                ordered_results.append(f"EXCEPTION: {str(result)}")
        
        success_count = sum(1 for r in ordered_results if not r.startswith("ERREUR"))
        self.logger.info(f"Batch termin√©: {success_count}/{len(prompts)} succ√®s")
        
        return ordered_results
    
    async def _make_api_call(self, payload: Dict[str, Any], max_retries: int = 3) -> str:
        """Effectue l'appel API avec retry automatique."""
        url = f"{self.groq_base_url}/chat/completions"
        
        for attempt in range(max_retries + 1):
            try:
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=60)) as session:
                    async with session.post(url, json=payload, headers=self.headers) as response:
                        
                        # Enregistrer la requ√™te pour rate limiting
                        self.request_timestamps.append(time.time())
                        
                        if response.status == 200:
                            data = await response.json()
                            content = data["choices"][0]["message"]["content"]
                            
                            # Validation de base
                            if not content or content.strip() == "":
                                raise LLMError("R√©ponse vide du mod√®le")
                            
                            return content.strip()
                            
                        elif response.status == 429:
                            # Rate limit atteint
                            retry_after = int(response.headers.get("retry-after", 60))
                            self.logger.warning(f"Rate limit atteint, attente {retry_after}s")
                            
                            if attempt < max_retries:
                                await asyncio.sleep(retry_after)
                                continue
                            else:
                                raise LLMRateLimitError("Limite de taux API d√©pass√©e")
                                
                        else:
                            # Autres erreurs HTTP
                            error_text = await response.text()
                            error_msg = f"Erreur API {response.status}: {error_text}"
                            
                            if attempt < max_retries:
                                self.logger.warning(f"{error_msg} - Tentative {attempt + 1}/{max_retries}")
                                await asyncio.sleep(2 ** attempt)  # Backoff exponentiel
                                continue
                            else:
                                raise LLMError(error_msg)
                                
            except asyncio.TimeoutError:
                if attempt < max_retries:
                    self.logger.warning(f"Timeout API - Tentative {attempt + 1}/{max_retries}")
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    raise LLMError("Timeout API apr√®s plusieurs tentatives")
                    
            except Exception as e:
                if attempt < max_retries:
                    self.logger.warning(f"Erreur r√©seau: {e} - Tentative {attempt + 1}/{max_retries}")
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    raise LLMError(f"Erreur de connexion: {str(e)}")
        
        raise LLMError("Toutes les tentatives ont √©chou√©")
    
    async def _check_rate_limits(self):
        """V√©rifie et applique les limites de taux."""
        current_time = time.time()
        
        # Nettoyer les timestamps anciens (plus de 1 minute)
        self.request_timestamps = [
            ts for ts in self.request_timestamps 
            if current_time - ts < 60
        ]
        
        # V√©rifier si on d√©passe la limite
        if len(self.request_timestamps) >= self.rate_limit_requests:
            oldest_request = min(self.request_timestamps)
            wait_time = 60 - (current_time - oldest_request)
            
            if wait_time > 0:
                self.logger.info(f"Rate limit: attente {wait_time:.1f}s")
                await asyncio.sleep(wait_time)
    
    def estimate_tokens(self, text: str) -> int:
        """Estime le nombre de tokens dans un texte."""
        # Approximation: 1 token ‚âà 4 caract√®res pour l'anglais/fran√ßais
        return len(text) // 4
    
    def validate_input_length(self, text: str, max_tokens: int = 6000) -> bool:
        """Valide que le texte ne d√©passe pas la limite de tokens."""
        estimated_tokens = self.estimate_tokens(text)
        return estimated_tokens <= max_tokens
    
    def truncate_text(self, text: str, max_tokens: int = 6000) -> str:
        """Tronque un texte pour respecter la limite de tokens."""
        estimated_tokens = self.estimate_tokens(text)
        
        if estimated_tokens <= max_tokens:
            return text
        
        # Calculer le ratio de troncature
        ratio = max_tokens / estimated_tokens
        target_length = int(len(text) * ratio * 0.9)  # Marge de s√©curit√©
        
        # Tronquer en pr√©servant les phrases
        sentences = text.split('. ')
        truncated = ""
        
        for sentence in sentences:
            if len(truncated) + len(sentence) + 2 <= target_length:
                truncated += sentence + ". "
            else:
                break
        
        self.logger.info(f"Texte tronqu√©: {len(text)} ‚Üí {len(truncated)} caract√®res")
        return truncated.strip()
    
    async def test_connection(self) -> bool:
        """Teste la connexion √† l'API."""
        try:
            result = await self.generate_completion(
                "Test de connexion. R√©ponds juste 'OK'.",
                system_prompt="Tu es un assistant de test."
            )
            
            if "ok" in result.lower():
                self.logger.info("Test de connexion LLM r√©ussi")
                return True
            else:
                self.logger.warning(f"Test de connexion √©trange: {result}")
                return False
                
        except Exception as e:
            self.logger.error(f"Test de connexion LLM √©chou√©: {e}")
            return False


class LLMManager:
    """
    Gestionnaire de services LLM avec strat√©gies multiples.
    """
    
    def __init__(self):
        self.logger = setup_logger("llm_manager")
        self.primary_service = LLMService()
        self.services = {
            "groq": self.primary_service
        }
    
    async def get_completion(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        service: str = "groq",
        **kwargs
    ) -> str:
        """
        Obtient une compl√©tion en utilisant le service sp√©cifi√©.
        
        Args:
            prompt: Prompt utilisateur
            system_prompt: Prompt syst√®me
            service: Service LLM √† utiliser
            **kwargs: Param√®tres suppl√©mentaires
            
        Returns:
            R√©ponse du mod√®le
        """
        if service not in self.services:
            raise ValueError(f"Service LLM inconnu: {service}")
        
        llm_service = self.services[service]
        return await llm_service.generate_completion(prompt, system_prompt, **kwargs)
    
    async def get_batch_completions(
        self,
        prompts: List[str],
        system_prompt: Optional[str] = None,
        service: str = "groq",
        **kwargs
    ) -> List[str]:
        """Obtient des compl√©tions en batch."""
        if service not in self.services:
            raise ValueError(f"Service LLM inconnu: {service}")
        
        llm_service = self.services[service]
        return await llm_service.generate_batch_completions(
            prompts, system_prompt, **kwargs
        )
    
    async def test_all_services(self) -> Dict[str, bool]:
        """Teste tous les services LLM disponibles."""
        results = {}
        
        for name, service in self.services.items():
            try:
                results[name] = await service.test_connection()
            except Exception as e:
                self.logger.error(f"Test service {name} √©chou√©: {e}")
                results[name] = False
        
        return results
    
# Exemple d'utilisation du service LLM

async def example_usage():
    """Exemple d'utilisation du service LLM."""
    
    # 1. Test de connexion simple
    print("=== Test de connexion ===")
    llm_service = LLMService()
    
    connection_ok = await llm_service.test_connection()
    print(f"Connexion LLM: {'‚úì OK' if connection_ok else '‚úó √âchec'}")
    
    if not connection_ok:
        print("Impossible de continuer sans connexion")
        return
    
    # 2. G√©n√©ration simple
    print("\n=== G√©n√©ration simple ===")
    try:
        response = await llm_service.generate_completion(
            prompt="Explique-moi en 2 phrases ce qu'est l'intelligence artificielle.",
            system_prompt="Tu es un expert en IA qui explique simplement."
        )
        print(f"R√©ponse: {response}")
    except Exception as e:
        print(f"Erreur: {e}")
    
    # 3. G√©n√©ration avec param√®tres personnalis√©s
    print("\n=== G√©n√©ration avec param√®tres ===")
    try:
        response = await llm_service.generate_completion(
            prompt="√âcris un haiku sur la technologie.",
            system_prompt="Tu es un po√®te sp√©cialis√© dans les haikus.",
            temperature=0.8,
            max_tokens=100
        )
        print(f"Haiku: {response}")
    except Exception as e:
        print(f"Erreur: {e}")
    
    # 4. G√©n√©ration en batch
    print("\n=== G√©n√©ration en batch ===")
    prompts = [
        "Qu'est-ce que Python?",
        "Qu'est-ce que JavaScript?",
        "Qu'est-ce que Rust?"
    ]
    
    try:
        responses = await llm_service.generate_batch_completions(
            prompts=prompts,
            system_prompt="R√©ponds en une phrase courte.",
            max_concurrent=2
        )
        
        for i, (prompt, response) in enumerate(zip(prompts, responses)):
            print(f"{i+1}. {prompt}")
            print(f"   ‚Üí {response}\n")
    except Exception as e:
        print(f"Erreur batch: {e}")
    
    # 5. Test des utilitaires
    print("\n=== Test des utilitaires ===")
    long_text = "Ceci est un texte tr√®s long. " * 1000
    print(f"Texte original: {len(long_text)} caract√®res")
    print(f"Tokens estim√©s: {llm_service.estimate_tokens(long_text)}")

    is_valid = llm_service.validate_input_length(long_text, max_tokens=7000)
    print(f"Texte valide (7000 tokens max): {is_valid}")
    
    if not is_valid:
        truncated = llm_service.truncate_text(long_text, max_tokens=7000)
        print(f"Texte tronqu√©: {len(truncated)} caract√®res")
        print(f"Contenu: {truncated[:200]}...")

# Test avec le gestionnaire LLM
async def example_manager_usage():
    """Exemple d'utilisation du gestionnaire LLM."""
    
    print("\n=== Test du gestionnaire LLM ===")
    
    manager = LLMManager()
    
    # Test de tous les services
    service_status = await manager.test_all_services()
    print("√âtat des services:")
    for service, status in service_status.items():
        print(f"  {service}: {'‚úì' if status else '‚úó'}")
    
    # Utilisation via le gestionnaire
    try:
        response = await manager.get_completion(
            prompt="Salut! Comment √ßa va?",
            system_prompt="Tu es un assistant amical.",
            service="groq"
        )
        print(f"\nR√©ponse du gestionnaire: {response}")
    except Exception as e:
        print(f"Erreur gestionnaire: {e}")

# Fonction principale pour tester
async def main():
    """Fonction principale de test."""
    try:
        await example_usage()
        await example_manager_usage()
    except KeyboardInterrupt:
        print("\n\nTest interrompu par l'utilisateur")
    except Exception as e:
        print(f"\nErreur inattendue: {e}")
        traceback.print_exc()

# Pour ex√©cuter le test
if __name__ == "__main__":
    print("üöÄ D√©marrage du test du service LLM...")
    asyncio.run(main())