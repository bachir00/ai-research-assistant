# üìö AI Research Assistant - Documentation Compl√®te

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![Python](https://img.shields.io/badge/python-3.12-green.svg)
![LangGraph](https://img.shields.io/badge/LangGraph-enabled-orange.svg)
![ChromaDB](https://img.shields.io/badge/ChromaDB-vectorstore-purple.svg)

## üéØ Vue d'ensemble

**AI Research Assistant** est un syst√®me intelligent de recherche et d'analyse documentaire utilisant LangGraph, plusieurs agents IA sp√©cialis√©s, et un syst√®me de m√©moire vectorielle avanc√©. Le syst√®me automatise l'ensemble du processus de recherche : de la collecte d'informations sur le web jusqu'√† la g√©n√©ration de rapports de synth√®se structur√©s.

### ‚ú® Fonctionnalit√©s principales

- üîç **Recherche web automatis√©e** avec extraction de mots-cl√©s intelligente
- üìÑ **Extraction de contenu** depuis des pages web avec parsing avanc√©
- üìù **G√©n√©ration de r√©sum√©s** d√©taill√©s et structur√©s
- üéØ **Synth√®se globale** avec analyse th√©matique transversale
- üíæ **Syst√®me de m√©moire** vectorielle et conversationnelle
- ü§ñ **Orchestration par LLM** via LangGraph
- üö´ **D√©duplication automatique** des documents
- ‚ö° **Cache intelligent** avec TTL configurable

---

## üèóÔ∏è Architecture du Projet

### Structure des dossiers

```
langGraphe-ai-research-assistant-main/
‚îÇ
‚îú‚îÄ‚îÄ config/                      # Configuration globale
‚îÇ   ‚îú‚îÄ‚îÄ settings.py             # Param√®tres de l'application
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py              # Templates de prompts
‚îÇ
‚îú‚îÄ‚îÄ src/                         # Code source principal
‚îÇ   ‚îú‚îÄ‚îÄ agents/                 # Agents sp√©cialis√©s
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py       # Agent de base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ researcher_agent.py # Recherche web
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_extractor_agent.py # Extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summarizer_agent.py # R√©sum√©s
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ global_synthesizer_agent.py # Synth√®se
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/               # Services partag√©s
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search_api.py      # APIs de recherche (Tavily, Serper)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_extraction.py # Extraction de contenu web
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py     # Service LLM (Groq)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_chunking.py   # D√©coupage de texte
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Mod√®les de donn√©es
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research_models.py  # Mod√®les de recherche
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_models.py  # Mod√®les de documents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ synthesis_models.py # Mod√®les de synth√®se
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ report_models.py    # Mod√®les de rapports
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ state_models.py     # √âtats du graphe
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ graph/                  # LangGraph
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nodes.py           # N≈ìuds du graphe
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/                   # Fonctionnalit√©s de base
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logging.py         # Configuration des logs
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ memory_system.py        # Syst√®me de m√©moire vectorielle
‚îÇ   ‚îú‚îÄ‚îÄ memory_integration.py   # Int√©gration de la m√©moire
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_system_prompt.py # Prompts avanc√©s
‚îÇ   ‚îî‚îÄ‚îÄ graph.py                # Graphe LangGraph principal
‚îÇ
‚îú‚îÄ‚îÄ tests/                       # Tests unitaires et d'int√©gration
‚îÇ   ‚îú‚îÄ‚îÄ test_researcher.py
‚îÇ   ‚îú‚îÄ‚îÄ test_content_extractor_agent.py
‚îÇ   ‚îú‚îÄ‚îÄ test_summarizer_agent.py
‚îÇ   ‚îî‚îÄ‚îÄ api_tests.py
‚îÇ
‚îú‚îÄ‚îÄ logs/                        # Fichiers de logs
‚îú‚îÄ‚îÄ .env                         # Variables d'environnement
‚îú‚îÄ‚îÄ requirements.txt             # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                    # Documentation principale
```

---

## üîß Architecture Technique

### Diagramme du Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         UTILISATEUR                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LLM ORCHESTRATEUR                            ‚îÇ
‚îÇ              (ChatGroq avec LangGraph)                          ‚îÇ
‚îÇ  ‚Ä¢ Analyse la requ√™te utilisateur                               ‚îÇ
‚îÇ  ‚Ä¢ D√©cide des outils √† utiliser                                 ‚îÇ
‚îÇ  ‚Ä¢ G√®re le flow de conversation                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                             ‚îÇ
            ‚ñº                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   RECHERCHE CACHE   ‚îÇ       ‚îÇ  NOUVELLE RECHERCHE ‚îÇ
‚îÇ                     ‚îÇ       ‚îÇ                     ‚îÇ
‚îÇ ‚Ä¢ V√©rif. cache 24h  ‚îÇ       ‚îÇ ‚Ä¢ Pipeline complet  ‚îÇ
‚îÇ ‚Ä¢ Recherche m√©moire ‚îÇ       ‚îÇ ‚Ä¢ 4 agents s√©quence ‚îÇ
‚îÇ ‚Ä¢ Topics similaires ‚îÇ       ‚îÇ ‚Ä¢ Stockage m√©moire  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                             ‚îÇ
           ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ            ‚îÇ
           ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SYST√àME DE M√âMOIRE                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  M√âMOIRE VECTORIELLE‚îÇ         ‚îÇ   M√âMOIRE AGENT      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  (ChromaDB)         ‚îÇ         ‚îÇ (Cache + Historique) ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ                     ‚îÇ         ‚îÇ                      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Embeddings        ‚îÇ         ‚îÇ ‚Ä¢ Conversations      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Recherche top-k   ‚îÇ         ‚îÇ ‚Ä¢ Cache recherches   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ D√©duplication     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚Ä¢ Topics + keywords  ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Persistance       ‚îÇ         ‚îÇ ‚Ä¢ Compression auto   ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   PIPELINE DE RECHERCHE                          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ   AGENT 1   ‚îÇ   ‚îÇ   AGENT 2   ‚îÇ   ‚îÇ   AGENT 3    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ RESEARCHER  ‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ  EXTRACTOR  ‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ SUMMARIZER   ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ             ‚îÇ   ‚îÇ             ‚îÇ   ‚îÇ              ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Tavily    ‚îÇ   ‚îÇ ‚Ä¢ Parsing   ‚îÇ   ‚îÇ ‚Ä¢ LLM        ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Serper    ‚îÇ   ‚îÇ ‚Ä¢ Nettoyage ‚îÇ   ‚îÇ ‚Ä¢ Chunking   ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Keywords  ‚îÇ   ‚îÇ ‚Ä¢ Validation‚îÇ   ‚îÇ ‚Ä¢ Points-cl√©s‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                             ‚îÇ                    ‚îÇ
‚îÇ                                             ‚ñº                    ‚îÇ
‚îÇ                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ                                    ‚îÇ     AGENT 4      ‚îÇ          ‚îÇ
‚îÇ                                    ‚îÇGLOBAL SYNTHESIZER‚îÇ          ‚îÇ
‚îÇ                                    ‚îÇ                  ‚îÇ          ‚îÇ
‚îÇ                                    ‚îÇ ‚Ä¢ Th√®mes         ‚îÇ          ‚îÇ
‚îÇ                                    ‚îÇ ‚Ä¢ Consensus      ‚îÇ          ‚îÇ
‚îÇ                                    ‚îÇ ‚Ä¢ Rapport final  ‚îÇ          ‚îÇ
‚îÇ                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
                   RAPPORT STRUCTUR√â
              (Markdown, HTML, Text, JSON)
```

---

## ü§ñ Description des Agents

### 1. üîç ResearcherAgent
**R√¥le** : Recherche web et extraction de mots-cl√©s

**Responsabilit√©s** :
- Extraction automatique de mots-cl√©s via LLM
- Recherche multi-API (Tavily, Serper)
- Filtrage et validation des r√©sultats
- Gestion du rate limiting

**Inputs** :
```python
ResearchQuery(
    topic: str,
    keywords: List[str],
    max_results: int = 10,
    search_depth: str = "basic"
)
```

**Outputs** :
```python
ResearchOutput(
    results: List[SearchResult],
    total_found: int,
    search_engine: str,
    search_time: float
)
```

### 2. üìÑ ContentExtractorAgent
**R√¥le** : Extraction et nettoyage du contenu web

**Responsabilit√©s** :
- Parsing HTML avec BeautifulSoup
- Nettoyage et normalisation du texte
- D√©tection du type de document
- Validation de la qualit√©

**Inputs** :
```python
ResearchOutput  # Provient du ResearcherAgent
```

**Outputs** :
```python
ExtractionResult(
    documents: List[Document],
    successful_extractions: int,
    failed_urls: List[str],
    extraction_stats: Dict
)
```

### 3. üìù SummarizerAgent
**R√¥le** : G√©n√©ration de r√©sum√©s d√©taill√©s

**Responsabilit√©s** :
- D√©coupage intelligent du texte (chunking)
- R√©sum√©s ex√©cutifs et d√©taill√©s
- Extraction de points-cl√©s et arguments
- Analyse de sentiment et cr√©dibilit√©

**Inputs** :
```python
ExtractionResult  # Provient du ContentExtractorAgent
```

**Outputs** :
```python
SummarizationOutput(
    summaries: List[DocumentSummary],
    total_documents: int,
    average_credibility: float,
    common_themes: List[str]
)
```

### 4. üéØ GlobalSynthesizerAgent
**R√¥le** : Synth√®se globale et g√©n√©ration de rapport

**Responsabilit√©s** :
- Analyse th√©matique transversale
- Identification de consensus et conflits
- G√©n√©ration de rapport structur√©
- Export multi-format (Markdown, HTML, Text)

**Inputs** :
```python
SummarizationOutput  # Provient du SummarizerAgent
```

**Outputs** :
```python
GlobalSynthesisOutput(
    final_report: FinalReport,
    synthesis_metadata: Dict,
    processing_stats: Dict,
    formatted_outputs: Dict[str, str]
)
```

---

## üíæ Syst√®me de M√©moire

### Architecture de la M√©moire

Le syst√®me utilise **deux types de m√©moire compl√©mentaires** :

#### 1. üóÑÔ∏è M√©moire Vectorielle (ChromaDB)

```python
VectorMemoryManager(
    persist_directory="./chroma_db",
    collection_name="research_documents",
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)
```

**Fonctionnalit√©s** :
- **Embeddings** : Mod√®les HuggingFace pour repr√©sentation vectorielle
- **Recherche s√©mantique** : Top-K avec scores de similarit√© cosinus
- **D√©duplication** : Hash MD5 pour √©viter les doublons
- **Persistance** : Stockage permanent sur disque
- **Nettoyage auto** : Suppression des documents > 30 jours

**M√©thodes principales** :
```python
# Ajout de documents
stats = vector_memory.add_documents(
    documents=[{
        'content': "...",
        'title': "...",
        'url': "..."
    }],
    source='research',
    check_duplicates=True
)

# Recherche s√©mantique
results = vector_memory.semantic_search(
    query="intelligence artificielle",
    k=5,
    filter_dict={'source': 'research'}
)

# Nettoyage
deleted = vector_memory.clear_old_documents(days=30)
```

#### 2. üß† M√©moire d'Agent (Cache + Historique)

```python
AgentMemoryManager(
    memory_file="./agent_memory.pkl",
    max_history=100,
    compression_threshold=50
)
```

**Fonctionnalit√©s** :
- **Historique conversationnel** : Deque avec limite (100 entr√©es)
- **Cache des recherches** : TTL 24h par d√©faut
- **Keywords tracking** : Association topic ‚Üí keywords
- **Compression auto** : Apr√®s 50 entr√©es
- **Persistance pickle** : Sauvegarde sur disque

**M√©thodes principales** :
```python
# Ajouter une conversation
agent_memory.add_conversation(
    user_message="R√©sume l'IA",
    assistant_response="...",
    metadata={'sources': 5}
)

# R√©cup√©rer du cache
result = agent_memory.get_research_result(
    topic="intelligence artificielle",
    max_age_hours=24
)

# Topics similaires
related = agent_memory.get_related_topics(
    topic="IA dans la sant√©",
    threshold=0.5
)
```

### üîó Syst√®me Int√©gr√©

```python
IntegratedMemorySystem()
```

Combine les deux m√©moires pour :
- Stockage automatique de tous les r√©sultats de recherche
- R√©cup√©ration intelligente du contexte
- V√©rification du cache avant nouvelle recherche
- Enrichissement des r√©ponses avec contexte historique

---

## üõ†Ô∏è Installation

### Pr√©requis

- **Python** : 3.12+
- **Pip** : version r√©cente
- **Git** : pour cloner le projet

### √âtapes d'installation

```bash
# 1. Cloner le projet
git clone https://github.com/votre-repo/ai-research-assistant.git
cd ai-research-assistant

# 2. Cr√©er un environnement virtuel
python -m venv venv

# Activer l'environnement
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# 3. Installer les d√©pendances
pip install -r requirements.txt

# 4. Configurer les variables d'environnement
cp .env.example .env
# √âditer .env avec vos cl√©s API
```

### Configuration `.env`

```env
# LLM
GROQ_API_KEY=your_groq_api_key_here

# Search APIs
TAVILY_API_KEY=your_tavily_api_key_here
SERPER_API_KEY=your_serper_api_key_here

# Optional
LOG_LEVEL=INFO
MAX_RETRIES=3
TIMEOUT=30
```

---
############################################################################
## üöÄ Utilisation

### Mode CLI Direct

```bash
# Recherche simple
python src/graph.py "impact de l'IA sur l'emploi"

# Mode test
python src/graph.py test

# Statistiques m√©moire
python src/graph.py stats
```

### Mode Interactif

```python
from src.graph import app_with_memory, run_test

# Lancer une recherche
run_test("R√©sume les √©nergies renouvelables", "Ma recherche")

# Ou utiliser directement le graphe
inputs = {"messages": [("user", "R√©sume l'IA dans la sant√©")]}
for state in app_with_memory.stream(inputs, stream_mode="values"):
    print(state["messages"][-1])
```

### Mode Menu Interactif

```bash
python tests/test_memory_system.py
```

Menu disponible :
```
1. Poser une question / Lancer une recherche
2. Rechercher dans la m√©moire
3. Voir l'historique
4. Statistiques de la m√©moire
5. Lancer la suite de tests
6. R√©initialiser la m√©moire
0. Quitter
```

### Int√©gration dans votre code

```python

from src.agents.researcher_agent import ResearcherAgent
from src.agents.content_extractor_agent import ContentExtractorAgent
from src.agents.summarizer_agent import SummarizerAgent
from src.agents.global_synthesizer_agent import GlobalSynthesizerAgent
from src.models.research_models import ResearchQuery

# Initialiser les agents
researcher = ResearcherAgent()
extractor = ContentExtractorAgent()
summarizer = SummarizerAgent()
synthesizer = GlobalSynthesizerAgent()

# Pipeline complet
async def recherche_complete(topic: str):
    # 1. Recherche
    query = ResearchQuery(
        topic=topic,
        keywords=await researcher.extract_keywords_with_llm(topic),
        max_results=5
    )
    research_data = await researcher.process(query)
    
    # 2. Extraction
    extraction_data = await extractor.process_from_research_output(
        research_output=research_data
    )
    
    # 3. R√©sum√©s
    summarization_data = await summarizer.process_from_extraction_result(
        extraction_result=extraction_data
    )
    
    # 4. Synth√®se
    synthesis = await synthesizer.process_from_summarization_output(
        summarization_output=summarization_data
    )
    
    return synthesis.final_report.formatted_outputs['markdown']
```

---

## üìä Exemples d'Utilisation

### Exemple 1 : Recherche Simple avec Cache

```python
# Premi√®re recherche (pipeline complet)
inputs = {
    "messages": [
        ("user", "R√©sume l'impact de l'IA sur le march√© du travail")
    ]
}

for state in app_with_memory.stream(inputs):
    print(state["messages"][-1].content)

# R√©sultat : Pipeline complet ex√©cut√©, r√©sultats mis en cache

# M√™me recherche 10 minutes apr√®s (utilise le cache)
inputs = {
    "messages": [
        ("user", "Rappelle-moi ce que tu as trouv√© sur l'IA et l'emploi")
    ]
}

for state in app_with_memory.stream(inputs):
    print(state["messages"][-1].content)

# R√©sultat : R√©ponse instantan√©e depuis le cache
```

### Exemple 2 : Recherche dans la M√©moire

```python
# Apr√®s plusieurs recherches sur l'IA
inputs = {
    "messages": [
        ("user", "Qu'as-tu trouv√© sur l'intelligence artificielle ?")
    ]
}

# Le LLM utilise automatiquement search_in_memory
# au lieu de lancer une nouvelle recherche web
```

### Exemple 3 : Historique et Statistiques

```python
from src.memory_system import memory_system

# Voir l'historique
history = list(memory_system.agent_memory.conversation_history)
for conv in history[-5:]:
    print(f"{conv['timestamp']}: {conv['user']}")

# Statistiques
print(f"Documents en m√©moire: {memory_system.vector_memory.collection.count()}")
print(f"Recherches en cache: {len(memory_system.agent_memory.research_cache)}")
```

### Exemple 4 : Recherche Approfondie

```python
from src.memory_integration import research_complete_pipeline_with_memory

# Recherche avec plus de sources
result = research_complete_pipeline_with_memory(
    topic="√©nergies renouvelables et transition √©cologique",
    max_results=10,  # Plus de sources
    use_cache=False  # Forcer une nouvelle recherche
)

print(result)  # Rapport Markdown complet
```

---


## üìù Logs et Monitoring

### Structure des logs

```
logs/
‚îú‚îÄ‚îÄ agent_researcher.log          # Recherche web
‚îú‚îÄ‚îÄ agent_content_extractor.log   # Extraction
‚îú‚îÄ‚îÄ agent_summarizer.log          # R√©sum√©s
‚îú‚îÄ‚îÄ agent_global_synthesizer.log  # Synth√®se
‚îú‚îÄ‚îÄ search_manager.log            # APIs de recherche
‚îú‚îÄ‚îÄ llm_service.log               # Appels LLM
‚îî‚îÄ‚îÄ complete_pipeline.log         # Pipeline complet
```

### Niveaux de log

```python
# Dans config/settings.py
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

### Exemple de logs

```
2025-11-18 10:30:15 | INFO | agent_researcher | Recherche pour: "IA emploi"
2025-11-18 10:30:18 | INFO | agent_researcher | Trouv√© 5 sources
2025-11-18 10:30:20 | INFO | agent_content_extractor | Extraction: 5/5 succ√®s
2025-11-18 10:30:45 | INFO | agent_summarizer | 5 r√©sum√©s g√©n√©r√©s
2025-11-18 10:31:10 | INFO | agent_global_synthesizer | Rapport: 1250 mots
2025-11-18 10:31:12 | INFO | memory_system | Stockage en m√©moire r√©ussi
```

---

## ‚öôÔ∏è Configuration Avanc√©e

### Personnaliser les prompts

```python
# config/prompts.py

CUSTOM_RESEARCH_PROMPT = """
Analyse approfondie sur {topic}.
Focus sur les aspects suivants :
- Impact √©conomique
- Implications sociales
- Perspectives futures
"""

# Utilisation
from config.prompts import CUSTOM_RESEARCH_PROMPT

prompt = CUSTOM_RESEARCH_PROMPT.format(topic="IA g√©n√©rative")
```

### Ajuster les param√®tres LLM

```python
# src/services/llm_service.py

class LLMService:
    def __init__(self):
        self.model = ChatGroq(
            model="llama-3.1-8b-instant",
            temperature=0.3,      # Cr√©ativit√© (0-1)
            max_tokens=2048,      # Longueur max
            top_p=0.9,           # Nucleus sampling
            frequency_penalty=0.5 # P√©nalit√© r√©p√©tition
        )
```

### Configurer la m√©moire vectorielle

```python
# src/memory_system.py

vector_memory = VectorMemoryManager(
    persist_directory="./custom_chroma_db",
    collection_name="my_research_docs",
    embedding_model="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  # Multilingue
)
```

---

## üîå Int√©gration API(Futur)

### REST API (FastAPI)

```python
# api/main.py
from fastapi import FastAPI
from src.graph import app_with_memory

app = FastAPI()

@app.post("/research")
async def research_endpoint(topic: str, max_results: int = 3):
    inputs = {"messages": [("user", f"R√©sume: {topic}")]}
    result = []
    
    for state in app_with_memory.stream(inputs):
        result.append(state["messages"][-1].content)
    
    return {"result": result[-1]}
```

### WebSocket (temps r√©el)

```python
from fastapi import WebSocket

@app.websocket("/ws/research")
async def websocket_research(websocket: WebSocket):
    await websocket.accept()
    
    while True:
        data = await websocket.receive_text()
        inputs = {"messages": [("user", data)]}
        
        for state in app_with_memory.stream(inputs):
            await websocket.send_text(
                state["messages"][-1].content
            )
```

---

## üêõ D√©pannage

### Probl√®mes courants

#### 1. Erreur de cl√© API manquante

```
ValueError: GROQ_API_KEY non d√©finie
```

**Solution** : V√©rifier le fichier `.env` et s'assurer que les cl√©s sont pr√©sentes.


#### 3. Rate limit atteint

```
WARNING | llm_service | Rate limit atteint, attente 12s
```

**Solution** : C'est normal, le syst√®me attend automatiquement. Pour √©viter :
- R√©duire `max_results`

#### 4. M√©moire satur√©e

```
MemoryError: Cannot allocate memory
```

**Solution** : Nettoyer la m√©moire :
```
memory_system.vector_memory.clear_old_documents(days=7)
```

---

```bash
# Build
docker build -t ai-research-assistant .

# Run
docker run -e GROQ_API_KEY=xxx -e TAVILY_API_KEY=yyy ai-research-assistant
```

### Production (Gunicorn)

```bash
gunicorn api.main:app --workers 4 --bind 0.0.0.0:8000
```

---

## üìà Roadmap

### Version 1.1 (En cours)
- [ ] Interface web avec Streamlit
- [ ] Support multilingue complet
- [ ] Export PDF des rapports
- [ ] Notifications par email

### Version 2.0 (Futur)
- [ ] Agents sp√©cialis√©s par domaine (sant√©, finance, tech)
- [ ] Int√©gration avec bases de donn√©es externes
- [ ] Syst√®me de fact-checking automatique
- [ ] API GraphQL

---

## ü§ù Contribution

Les contributions sont les bienvenues !

---

## üë• Auteurs

- **Bachir** - *D√©veloppeur Principal* - [GitHub](https://github.com/bachir00)

---

## üôè Remerciements

- LangChain & LangGraph pour le framework
- Groq pour l'acc√®s aux LLMs
- ChromaDB pour le stockage vectoriel
- Tavily & Serper pour les APIs de recherche
- La communaut√© open-source

---

## üìû Support

- üìß Email : bassiroukane@esp.sn